{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "# import tqdm\n",
    "import time\n",
    "import datetime\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision.io import read_image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import struct\n",
    "from functools import reduce\n",
    "import shelve\n",
    "import shutil\n",
    "from glob import glob\n",
    "import os, random\n",
    "from shutil import copyfile,rmtree\n",
    "\n",
    "import tensorflow.lite as tflite\n",
    "import scipy.io as sio\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_map(ch_map):\n",
    "    tmp = ch_map.cpu().detach().numpy().squeeze()\n",
    "    sio.savemat('./map.mat',{'Z':tmp})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, ksize, stride, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels=in_ch, out_channels=out_ch, \n",
    "                              kernel_size=ksize,stride=stride)\n",
    "        self.conv_bn = nn.BatchNorm1d(out_ch)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(out_ch, out_ch//reduction, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_ch//reduction, out_ch, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.conv_bn(x)\n",
    "        b, c, l = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1)\n",
    "        return x * y.expand_as(x),y\n",
    "\n",
    "class CLS_NET(nn.Module):\n",
    "    def __init__(self,num_classes=2):\n",
    "        super(CLS_NET, self).__init__()\n",
    "        self.head_avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(48, 48//16, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(48//16, 48, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.layers = [\n",
    "            [48,48,7,2,2],\n",
    "#             [48,24,5,2,4 ],        \n",
    "            [48,8, 3,2,4 ],  \n",
    "        ]\n",
    "        self.basicblock = []\n",
    "        for in_ch, out_ch, ksize, stride, reduction in self.layers:\n",
    "            self.basicblock.append(SELayer(in_ch, out_ch, ksize, stride, reduction))\n",
    "        self.basicblock = nn.ModuleList(self.basicblock)     \n",
    "        \n",
    "        self.fc1 = nn.Linear(496, 32)\n",
    "        self.bn1 = nn.BatchNorm1d(32)\n",
    "        self.relu1 = nn.ReLU6(inplace=True)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc2 = nn.Linear(32, 16)\n",
    "        self.bn2 = nn.BatchNorm1d(16)\n",
    "        self.relu2 = nn.ReLU6(inplace=True)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc3 = nn.Linear(16, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         print(x.shape)\n",
    "        b, c, l = x.size()\n",
    "        y = self.head_avgpool(x).view(b, c)\n",
    "        x = self.head(y).view(b, c, 1).expand_as(x)*x\n",
    "\n",
    "        x,ch_map = self.basicblock[0](x) #256 to 128\n",
    "#         print(x.shape)\n",
    "        x,_ = self.basicblock[1](x) #128 to 64\n",
    "#         print(x.shape)\n",
    "#         x,_ = self.basicblock[2](x) #64 to 32\n",
    "#         print(x.shape)\n",
    "        x = x.reshape((-1,496))\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.softmax(x)\n",
    "        return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(data_list,batch_size=24):\n",
    "#     data_list (24, 48, 256)\n",
    "    label = list(range(data_list.shape[0]))\n",
    "    if SEED is not None:\n",
    "        random.seed(SEED)\n",
    "        random.shuffle(label)\n",
    "    \n",
    "    batch = torch.zeros((batch_size,48,256))\n",
    "    for i in range(batch_size):\n",
    "        batch[i,:] = data_list[label[i],:,:]\n",
    "    \n",
    "    label = np.array(label)\n",
    "    label[label<12] = 0\n",
    "    label[label>=12] = 1\n",
    "    label = torch.tensor(label)\n",
    "    marks = label.long().to(device)\n",
    "    return batch,marks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distiller_epoch(stu_model, criterion, optimizer, scheduler, train_mode, show_tqdm=True):\n",
    "    if train_mode:\n",
    "        batch,marks = data_loader(data_cube,24)\n",
    "        stu_model.train()\n",
    "    else:\n",
    "        batch,marks = data_loader(data_cube,24)\n",
    "        stu_model.eval()\n",
    "\n",
    "    loss_list = []\n",
    "    acc_list = []\n",
    "    loss_tmp = 0\n",
    "    acc_n = 0\n",
    "    \n",
    "    stu_inputs = batch.to(device)\n",
    "    with torch.set_grad_enabled(train_mode):\n",
    "        stu_outputs,ch_map = stu_model(stu_inputs)\n",
    "    \n",
    "    loss = criterion(stu_outputs,marks).to(device)\n",
    "    \n",
    "    stu_mark = torch.argmax(stu_outputs,dim=1)\n",
    "    for i in range(BATCH_size):\n",
    "        if stu_mark[i].item() == marks[i].item():\n",
    "            acc_n+=1\n",
    "    \n",
    "    if train_mode:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    loss_list += [loss.item()]\n",
    "    loss_tmp  +=  loss.item()\n",
    "        \n",
    "#     this_epoch_loss = loss_tmp / batchnumber\n",
    "    acc_rate = acc_n/(BATCH_size)\n",
    "#     print(loss_list,acc_rate)\n",
    "    return loss_list,acc_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distiller_train(stu_model,epochs,base_lr=0.01, max_lr=0.1,step_size_up=500, step_size_down=500,gamma=1,\n",
    "                    weight_decay=1e-5, save_path='./stu_model.pt',\n",
    "                    criterion=nn.CrossEntropyLoss(), show_tqdm=True,\n",
    "                   all_train_losses=[],all_val_losses=[],all_learn_rate=[],all_rkd=[]):\n",
    "        \n",
    "    global EPOCH_idx  # global epoch for tensorboard record index \n",
    "    # 优化器\n",
    "    optimizer = torch.optim.SGD(stu_model.parameters(),\n",
    "                                lr=base_lr, momentum=0.9, dampening=0, \n",
    "                                weight_decay=weight_decay, nesterov=True)\n",
    "    # 学习率更新策略\n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=base_lr, max_lr=max_lr,\n",
    "                                                  step_size_up=step_size_up, step_size_down=step_size_down,\n",
    "                                                  mode='exp_range',gamma=gamma)\n",
    "    best_a = 0\n",
    "    # 逐epoch训练\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "#         print('EPOCH #', epoch+1,end=' ')\n",
    "#         print('LR: {:0.6f}'.format(optimizer.param_groups[-1]['lr']),end=' ')\n",
    "        loss_list_t,a_t = distiller_epoch(stu_model, criterion, optimizer, scheduler,\n",
    "                                      train_mode=True, show_tqdm=show_tqdm)\n",
    "        loss_list_v,a_v = distiller_epoch(stu_model, criterion, optimizer, scheduler,\n",
    "                                      train_mode=False, show_tqdm=show_tqdm)\n",
    "        all_train_losses += [np.mean(loss_list_t[0])]\n",
    "        all_val_losses += [np.mean(loss_list_v[0])]\n",
    "        all_learn_rate += [optimizer.param_groups[-1]['lr']]\n",
    "        all_rkd += [a_t]\n",
    "#         print(all_rkd)\n",
    "        if a_v > best_a:\n",
    "            best_a = a_v\n",
    "            torch.save(stu_model.state_dict(), save_path)      # epoch的验证loss小于最优loss 保存模型\n",
    "#             long_pic_tensorboard(val_set_marked,stu_model,col=25,row=16,epochidx=EPOCH_idx)\n",
    "\n",
    "#         torch.save(stu_model.state_dict(), save_path)        # 每轮都保存模型\n",
    "        EPOCH_idx+=1\n",
    "        \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distiller_train_lr_test(stu_model,epochs,init_lr,lr_rate, times,\n",
    "                            weight_decay=1e-5,batch_size=128, save_path='./stu_model.pt',\n",
    "                            criterion=nn.CrossEntropyLoss(), show_tqdm=True,\n",
    "                            all_train_losses=[],all_val_losses=[],all_learn_rate=[],all_rkd=[]):\n",
    "    '''训练调度器  \n",
    "    criterion  切换损失函数模式 \n",
    "    auto_lr    自动更新学习率开关'''\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    # 优化器\n",
    "    optimizer = torch.optim.SGD(stu_model.parameters(),\n",
    "                                lr=init_lr, momentum=0.9, dampening=0, \n",
    "                                weight_decay=weight_decay, nesterov=True)\n",
    "    \n",
    "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=init_lr, max_lr=init_lr,\n",
    "                                                  step_size_up=100, step_size_down=100,\n",
    "                                                  mode='exp_range',gamma=1)\n",
    "    \n",
    "    # 逐epoch训练\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        print('EPOCH #', epoch+1,end=' ')\n",
    "        print('LR:',init_lr)\n",
    "        tmp = []\n",
    "        for i in range(times):\n",
    "            loss_list_t = distiller_epoch(stu_model, criterion, optimizer, scheduler,\n",
    "                                      train_mode=True, show_tqdm=show_tqdm)\n",
    "            tmp+=loss_list_t[0]\n",
    "            \n",
    "        all_train_losses += [np.mean(tmp)]                   # 拼接进所有训练loss列表 逐batch\n",
    "        all_learn_rate   += [init_lr]                        # 拼接当前学习率\n",
    "\n",
    "        if np.mean(tmp)>200 or np.isnan(np.mean(tmp)):\n",
    "            print(np.mean(tmp))\n",
    "            break\n",
    "        init_lr *= lr_rate\n",
    "        for param_group in optimizer.param_groups:   # 更新所有lr\n",
    "            param_group['lr'] = init_lr\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_mat(path):\n",
    "    data = sio.loadmat(path)\n",
    "    data_cube = torch.tensor(data['sig2'].squeeze().transpose((2,0,1)))\n",
    "    return data_cube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lr test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch=2000\n",
    "# start_lr=1e-6\n",
    "# factor=1.1\n",
    "# times=1\n",
    "# # exp test lr \n",
    "# # print('start exp test >>> from {} to {}'.format(start_lr,start_lr*(factor**epoch)))\n",
    "# t_loss, v_loss, l_rate, rkds=[],[],[],[]\n",
    "\n",
    "# distiller_train_lr_test(stu_model,epoch,start_lr,factor,times,\n",
    "#                         weight_decay=1e-6,batch_size=BATCH_size, show_tqdm=False,\n",
    "#                         all_train_losses=t_loss,all_val_losses=v_loss,\n",
    "#                         all_learn_rate=l_rate,all_rkd=rkds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# %pylab inline\n",
    "\n",
    "# #plot test result\n",
    "# x = [start_lr*(factor**i) for i in range(len(t_loss))]\n",
    "# plt.plot(x,np.array(t_loss))\n",
    "# plt.xscale('log')\n",
    "# # plt.ylim((0, 2))\n",
    "# # plt.xlim((1e0, 10**0.2))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './back3_HBR_proc.mat'\n",
    "stu_model = CLS_NET()\n",
    "SEED = 1024\n",
    "BATCH_size = 24\n",
    "EPOCH_idx = 0           # log epoch index init to 0\n",
    "data_cube = load_data_from_mat(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[24, 48, 1]' is invalid for input of size 432",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-20f8c62d21d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_cube\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-b59c4c754663>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_avgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mch_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicblock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#256 to 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[24, 48, 1]' is invalid for input of size 432"
     ]
    }
   ],
   "source": [
    "batch,marks = data_loader(data_cube,24)\n",
    "res = stu_model(batch)\n",
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d569fc153dfb49d993816912d1f6d1ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=400.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[24, 48, 1]' is invalid for input of size 432",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-c66504c356b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mt_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrkds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m distiller_train(stu_model,epoch_n,base_lr=10**(-5)/lrs, max_lr=10**(-2)/lrs,step_size_up=step, step_size_down=step,gamma=1,\n\u001b[0m\u001b[1;32m      9\u001b[0m                 \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mshow_tqdm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_train_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_val_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mv_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                         all_learn_rate=l_rate,all_rkd=rkds)\n",
      "\u001b[0;32m<ipython-input-27-c7b02d113288>\u001b[0m in \u001b[0;36mdistiller_train\u001b[0;34m(stu_model, epochs, base_lr, max_lr, step_size_up, step_size_down, gamma, weight_decay, save_path, criterion, show_tqdm, all_train_losses, all_val_losses, all_learn_rate, all_rkd)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#         print('EPOCH #', epoch+1,end=' ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#         print('LR: {:0.6f}'.format(optimizer.param_groups[-1]['lr']),end=' ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         loss_list_t,a_t = distiller_epoch(stu_model, criterion, optimizer, scheduler,\n\u001b[0m\u001b[1;32m     21\u001b[0m                                       train_mode=True, show_tqdm=show_tqdm)\n\u001b[1;32m     22\u001b[0m         loss_list_v,a_v = distiller_epoch(stu_model, criterion, optimizer, scheduler,\n",
      "\u001b[0;32m<ipython-input-26-18873d8f4909>\u001b[0m in \u001b[0;36mdistiller_epoch\u001b[0;34m(stu_model, criterion, optimizer, scheduler, train_mode, show_tqdm)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mstu_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mstu_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mch_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstu_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstu_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstu_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmarks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-b59c4c754663>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_avgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mch_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasicblock\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#256 to 128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[24, 48, 1]' is invalid for input of size 432"
     ]
    }
   ],
   "source": [
    "epoch_n = 400\n",
    "cycle_n = 8\n",
    "# lrs = 1e4/8\n",
    "lrs=1\n",
    "step = epoch_n*(24*2//BATCH_size)/(cycle_n*2)\n",
    "\n",
    "t_loss, v_loss, l_rate, rkds=[],[],[],[]\n",
    "distiller_train(stu_model,epoch_n,base_lr=10**(-5)/lrs, max_lr=10**(-2)/lrs,step_size_up=step, step_size_down=step,gamma=1,\n",
    "                weight_decay=1e-6,show_tqdm=False,all_train_losses=t_loss,all_val_losses=v_loss,\n",
    "                        all_learn_rate=l_rate,all_rkd=rkds)\n",
    "plt.plot(t_loss)\n",
    "plt.plot(rkds)\n",
    "plt.plot(l_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stu_model.load_state_dict(torch.load('./stu_model.pt')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = None\n",
    "data_cube = load_data_from_mat(path)\n",
    "batch,marks = data_loader(data_cube,24)\n",
    "res_sort,ch_map = stu_model(batch)\n",
    "save_map(ch_map)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
